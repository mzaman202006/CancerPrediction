{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Predicting Breast Cancer_ANN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPyBArPCSojObx/R6WTNT5l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mzaman202006/CancerPrediction/blob/main/Predicting_Breast_Cancer_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGYMK6Xj0BS4",
        "outputId": "c93f32b5-d333-46a8-eda1-b2431eb07ad7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'smoothness_mean',\n",
              "       'compactness_mean', 'concavity_mean', 'concave points_mean',\n",
              "       'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se',\n",
              "       'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se',\n",
              "       'symmetry_se', 'fractal_dimension_se'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score,recall_score,f1_score\n",
        "# Importing data\n",
        "df = pd.read_csv('data.csv')\n",
        "del df['Unnamed: 32']\n",
        "\n",
        "#del df['id']\n",
        "# first, drop all \"worst\" columns\n",
        "cols = ['radius_worst', \n",
        "        'texture_worst', \n",
        "        'perimeter_worst', \n",
        "        'area_worst', \n",
        "        'smoothness_worst', \n",
        "        'compactness_worst', \n",
        "        'concavity_worst',\n",
        "        'concave points_worst', \n",
        "        'symmetry_worst', \n",
        "        'fractal_dimension_worst']\n",
        "df = df.drop(cols, axis=1)\n",
        "\n",
        "# then, drop all columns related to the \"perimeter\" and \"area\" attributes\n",
        "cols = ['perimeter_mean',\n",
        "        'perimeter_se', \n",
        "        'area_mean', \n",
        "        'area_se']\n",
        "df = df.drop(cols, axis=1)\n",
        "df.columns\n",
        "\n",
        "# verify remaining columns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "-QGqvQOO7wbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OJUlVid37vwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding categorical data\n",
        "X = df.iloc[:, 2:].values\n",
        "y = df.iloc[:, 1].values\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_X_1 = LabelEncoder()\n",
        "y = labelencoder_X_1.fit_transform(y)\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
        "\n",
        "#Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "metadata": {
        "id": "C_-fzQ2-7pb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "classifier = Sequential()"
      ],
      "metadata": {
        "id": "3ZkypnuG4KQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialising the ANN\n"
      ],
      "metadata": {
        "id": "BH9U6-kG4FlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the input layer and the first hidden layer\n",
        "classifier.add(Dense(units=8, kernel_initializer='normal', activation='relu', input_dim=16))\n",
        "# Adding dropout to prevent overfitting\n",
        "classifier.add(Dropout(rate=0.1))\n",
        "\n",
        "# Adding the second hidden layer\n",
        "classifier.add(Dense(units=8, kernel_initializer='normal', activation='relu'))\n",
        "# Adding dropout to prevent overfitting\n",
        "classifier.add(Dropout(rate=0.1))\n",
        "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# Adding the output layer\n",
        "classifier.add(Dense(units=1, kernel_initializer='normal', activation='sigmoid'))\n",
        "# Fitting the ANN to the Training set\n",
        "classifier.fit(X_train, y_train, batch_size=100, epochs=150)"
      ],
      "metadata": {
        "id": "g_A4F6TP4Zsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Our accuracy is {}%\".format(((cm[0][0] + cm[1][1])/57)*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUmHfWER7I7s",
        "outputId": "ea6f2a5c-1310-42e3-e152-64c6f806c39f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our accuracy is 94.73684210526315%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision quantifies the number of positive class predictions that actually belong to the positive class.\n",
        "precision = precision_score(y_test, y_pred, average='binary')\n",
        "\n",
        "# Recall quantifies the number of positive class predictions made out of all positive examples in the dataset.\n",
        "recall = recall_score(y_test, y_pred, average='binary')\n",
        "\n",
        "# F-Measure provides a single score that balances both the concerns of precision and recall.\n",
        "score = f1_score(y_test, y_pred, average='binary')\n",
        "\n",
        "print('Precision : ',precision *100 ,' %')\n",
        "print('------------------------')\n",
        "print('Recall : ' ,recall*100 ,' %') # Best score to considerate in our case beacause Recall is appropriate when minimizing False Negatives. \n",
        "print('------------------------')\n",
        "print('F1_score : ', score*100 ,' %')"
      ],
      "metadata": {
        "id": "duNXw7177LTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82f79d13-57af-42e5-9342-b6ebd6f779e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision :  91.30434782608695  %\n",
            "------------------------\n",
            "Recall :  95.45454545454545  %\n",
            "------------------------\n",
            "F1_score :  93.33333333333333  %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ISgJCImo7PWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(cm,annot=True)\n",
        "plt.savefig('h.png')"
      ],
      "metadata": {
        "id": "fOgFPAsa7RWs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}